// twentyQ.js - ìŠ¤ë¬´ê³ ê°œ (20 Questions) Game

// You should move OpenAI API calls to the backend for security in production!
const OPENAI_API_KEY = window.OPENAI_API_KEY;
let questionCount = 0;
let maxQuestions = 20;
let gameOver = false;
let aiWord = "";
let messages = [
  {
    role: "system",
    content: "ë„ˆëŠ” ìŠ¤ë¬´ê³ ê°œ ê²Œì„ì˜ AI ë§ˆìŠ¤í„°ì•¼. ë°˜ë“œì‹œ ì´ˆë“±í•™ìƒ(íŠ¹íˆ 3~6í•™ë…„)ì´ ì˜ ì•Œê³  ì‰½ê²Œ ë§í ìˆ˜ ìˆëŠ” ëª…ì‚¬(ì‚¬ë¬¼, ë™ë¬¼, ìŒì‹, ìºë¦­í„° ë“±)ë§Œ ìƒê°í•´. ë„ˆë¬´ ì–´ë µê±°ë‚˜ ìƒì†Œí•œ ë‹¨ì–´, ì–´ë¥¸ë§Œ ì•„ëŠ” ë‹¨ì–´, ì™¸ë˜ì–´ë‚˜ ì „ë¬¸ìš©ì–´ëŠ” ì ˆëŒ€ ê³ ë¥´ì§€ ë§ˆ. ì‚¬ìš©ìê°€ ì˜ˆ/ì•„ë‹ˆì˜¤ë¡œ ëŒ€ë‹µí•  ìˆ˜ ìˆëŠ” ì§ˆë¬¸ì„ í•˜ë©´ ì¹œì ˆí•˜ê²Œ 'ì˜ˆ' ë˜ëŠ” 'ì•„ë‹ˆì˜¤'ë¡œë§Œ ëŒ€ë‹µí•´. ë§Œì•½ ì‚¬ìš©ìê°€ ì˜ˆ/ì•„ë‹ˆì˜¤ë¡œ ëŒ€ë‹µí•  ìˆ˜ ì—†ëŠ” ì§ˆë¬¸(ì˜ˆ: 'ì´ê²Œ ë­ì•¼?', 'ìƒ‰ê¹”ì´ ë­ì•¼?', 'í¬ê¸°ê°€ ì–´ë–»ê²Œ ë¼?')ì„ í•˜ë©´, 'ìŠ¤ë¬´ê³ ê°œëŠ” ì˜ˆ ë˜ëŠ” ì•„ë‹ˆì˜¤ë¡œ ëŒ€ë‹µí•  ìˆ˜ ìˆëŠ” ì§ˆë¬¸ë§Œ í•  ìˆ˜ ìˆì–´! ë‹¤ì‹œ ì§ˆë¬¸í•´ì¤˜.'ë¼ê³  ë§í•´ì¤˜. ì´ëŸ° ê²½ìš° ì§ˆë¬¸ íšŸìˆ˜ëŠ” ì„¸ì§€ ì•Šì•„. 20ë²ˆ ì´ë‚´ì— ì‚¬ìš©ìê°€ ì •ë‹µì„ ë§íˆë©´ 'ì •ë‹µì´ì•¼! ì¶•í•˜í•´!'ë¼ê³  í•˜ê³ , 20ë²ˆì´ ì§€ë‚˜ë©´ 'ì•„ì‰½ì§€ë§Œ ì •ë‹µì€ [ë‹¨ì–´]ì˜€ì–´!'ë¼ê³  ì•Œë ¤ì¤˜. ì •ë‹µì„ ë§íˆê¸° ì „ê¹Œì§€ëŠ” ë‹¨ì–´ë¥¼ ì ˆëŒ€ ì§ì ‘ ë§í•˜ì§€ ë§ˆ. ëª¨ë“  ëŒ€í™”ëŠ” ë°˜ë§ë¡œ í•´ì¤˜."
  }
];

window.onload = function() {
  // OpenAI Whisper Speech Recognition Setup with M Key Push-to-Talk
  let isRecording = false;
  let mediaRecorder = null;
  let audioChunks = [];
  let stream = null;
  let speechCallback = null;

  // M key push-to-talk functionality
  document.addEventListener('keydown', async (event) => {
    if (event.code === 'KeyM' && !isRecording && speechCallback) {
      event.preventDefault();
      await startSpeechRecording();
    }
  });

  document.addEventListener('keyup', async (event) => {
    if (event.code === 'KeyM' && isRecording) {
      event.preventDefault();
      await stopSpeechRecording();
    }
  });

  async function startSpeechRecording() {
    try {
      isRecording = true;
      audioChunks = [];
      
      voiceBtn.textContent = 'ğŸ™ï¸ ë…¹ìŒ ì¤‘... (Mí‚¤ë¥¼ ë†“ìœ¼ë©´ ì¸ì‹ ì‹œì‘)';
      voiceBtn.disabled = true;
      
      stream = await navigator.mediaDevices.getUserMedia({ 
        audio: {
          channelCount: 1,
          sampleRate: 44100,
          sampleSize: 16
        } 
      });
      
      mediaRecorder = new MediaRecorder(stream, {
        mimeType: 'audio/webm'
      });
      
      mediaRecorder.ondataavailable = (event) => {
        if (event.data.size > 0) {
          audioChunks.push(event.data);
        }
      };
      
      mediaRecorder.start();
      
    } catch (err) {
      console.error('Error starting recording:', err);
      voiceBtn.textContent = 'ë…¹ìŒ ì˜¤ë¥˜';
      voiceBtn.disabled = false;
      isRecording = false;
    }
  }

  async function stopSpeechRecording() {
    if (!isRecording || !mediaRecorder) return;
    
    isRecording = false;
    
    return new Promise((resolve) => {
      mediaRecorder.onstop = async () => {
        try {
          voiceBtn.textContent = 'ìŒì„± ì¸ì‹ ì¤‘...';
          
          const audioBlob = new Blob(audioChunks, { type: 'audio/webm' });
          const arrayBuffer = await audioBlob.arrayBuffer();
          const buffer = window.electron.bufferFrom(new Uint8Array(arrayBuffer));
          
          const text = await window.electron.ipcRenderer.invoke('recognize-audio', buffer);
          
          voiceBtn.textContent = 'ğŸ¤ Mí‚¤ë¥¼ ëˆŒëŸ¬ì„œ ì§ˆë¬¸';
          voiceBtn.disabled = false;
          
          if (speechCallback) {
            speechCallback(text.trim());
            speechCallback = null;
          }
          
        } catch (err) {
          console.error('Speech recognition error:', err);
          voiceBtn.textContent = 'ğŸ¤ Mí‚¤ë¥¼ ëˆŒëŸ¬ì„œ ì§ˆë¬¸';
          voiceBtn.disabled = false;
        } finally {
          if (stream) {
            stream.getTracks().forEach(track => track.stop());
            stream = null;
          }
          resolve();
        }
      };
      
      mediaRecorder.stop();
    });
  }

  // Remove the text input and button if present
  const questionForm = document.getElementById('questionForm');
  questionForm.innerHTML = '';

  // Add the voice input button
  const voiceBtn = document.createElement('button');
  voiceBtn.type = 'button';
  voiceBtn.textContent = 'ğŸ¤ Mí‚¤ë¥¼ ëˆŒëŸ¬ì„œ ì§ˆë¬¸';
  voiceBtn.style.margin = '12px 0 0 0';
  voiceBtn.style.fontSize = '1.1em';
  voiceBtn.disabled = true; // Initially disabled until user presses M
  questionForm.appendChild(voiceBtn);

  // Add transcript display
  const transcriptDisplay = document.createElement('div');
  transcriptDisplay.id = 'voiceInputDisplay';
  transcriptDisplay.style = 'font-size:1.1em;color:#1976d2;margin:12px 0 0 0;min-height:28px;';
  transcriptDisplay.textContent = 'M í‚¤ë¥¼ ëˆ„ë¥´ê³  ìˆëŠ” ë™ì•ˆ ì§ˆë¬¸í•˜ì„¸ìš”';
  questionForm.appendChild(transcriptDisplay);

  // Set up speech callback for questions
  function setupSpeechInput() {
    speechCallback = (transcript) => {
      transcriptDisplay.textContent = `ì§ˆë¬¸: ${transcript}`;
      processQuestion(transcript);
    };
  }

  // Initialize speech input
  setupSpeechInput();

  async function processQuestion(transcript) {
    transcriptDisplay.textContent = 'ì…ë ¥: ' + transcript;
    
    if (gameOver) return;
    
    // First, send to AI without incrementing question count
    messages.push({ role: 'user', content: transcript });
    document.getElementById('aiAnswer').textContent = 'AIê°€ ìƒê° ì¤‘...';
    
    try {
      // Get API key via IPC
      const apiKey = await window.electron.ipcRenderer.invoke('get-openai-key');
      if (!apiKey) {
        throw new Error('OpenAI API key not available');
      }

      const response = await fetch("https://api.openai.com/v1/chat/completions", {
        method: "POST",
        headers: {
          "Authorization": `Bearer ${apiKey}`,
          "Content-Type": "application/json"
        },
        body: JSON.stringify({
          model: "gpt-3.5-turbo",
          messages: messages
        })
      });

      if (!response.ok) {
        throw new Error(`OpenAI API error: ${response.status}`);
      }

      const data = await response.json();
      let reply = data.choices[0].message.content.trim();
      
      // Remove emojis
      const emojiRegex = /(\u00a9|\u00ae|[\u2000-\u3300]|\ud83c[\ud000-\udfff]|\ud83d[\ud000-\udfff]|\ud83e[\ud000-\udfff])/g;
      reply = reply.replace(emojiRegex, '');
      
      messages.push({ role: 'assistant', content: reply });
      document.getElementById('aiAnswer').textContent = reply;
      
      // Increment question count
      questionCount++;
      document.getElementById('currentQuestion').textContent = questionCount;
      
      // Check for game end conditions
      if (questionCount >= 20) {
        document.getElementById('finalResult').textContent = '20ê°œ ì§ˆë¬¸ì´ ëª¨ë‘ ëë‚¬ìŠµë‹ˆë‹¤! ì •ë‹µì„ ë§í˜€ë³´ì„¸ìš”!';
        endGame();
      }
      
      // Reset for next question
      setupSpeechInput();
      
    } catch (err) {
      console.error('Error processing question:', err);
      document.getElementById('aiAnswer').textContent = 'ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: ' + err.message;
    }
  }

  document.getElementById('questionForm').onsubmit = function(e) {
    e.preventDefault();
    // Disable text input submission
    return false;
  };
  
  document.getElementById('restartBtn').onclick = function() {
    questionCount = 0;
    gameOver = false;
    messages = [messages[0]];
    document.getElementById('currentQuestion').textContent = questionCount;
    document.getElementById('aiAnswer').textContent = '';
    document.getElementById('finalResult').textContent = '';
    transcriptDisplay.textContent = 'M í‚¤ë¥¼ ëˆ„ë¥´ê³  ìˆëŠ” ë™ì•ˆ ì§ˆë¬¸í•˜ì„¸ìš”';
    document.getElementById('restartBtn').style.display = 'none';
    setupSpeechInput();
  };
};

function endGame() {
  gameOver = true;
  speechCallback = null; // Disable speech input
  document.getElementById('restartBtn').style.display = 'inline-block';
}
