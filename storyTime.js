// storyTime.js - Voice-based Story Time Activity with OpenAI API
const OPENAI_API_KEY = window.OPENAI_API_KEY;

const storyMessages = [
  { role: "system", content: "ë„ˆëŠ” ì´ˆë“±í•™ìƒì„ ìœ„í•œ ì°½ì˜ì ì¸ ì´ì•¼ê¸° ì„ ìƒë‹˜ì´ì•¼. ì‚¬ìš©ìê°€ ì´ì–´ì„œ ì“¸ ìˆ˜ ìˆë„ë¡ ì¬ë¯¸ìˆê³  ì§§ì€ ì´ì•¼ê¸°ì˜ ì²« ë¬¸ë‹¨ì„ ë§Œë“¤ì–´ì¤˜. ì‚¬ìš©ìê°€ ì´ì–´ì„œ ì“´ ë¬¸ì¥ì„ ë°›ìœ¼ë©´, ê·¸ ë‹¤ìŒ ë¬¸ë‹¨ì„ ì´ì–´ì„œ ì¨ì¤˜. ëª¨ë“  ëŒ€í™”ëŠ” í•œêµ­ì–´ë¡œ ì§„í–‰í•´. ì¤‘ìš”í•œ ê·œì¹™ë“¤: 1) ëª¨ë“  ë¬¸ì¥ì€ ì¡´ëŒ“ë§ë¡œ ì¨ì¤˜ (ì˜ˆ: ~í–ˆì–´ìš”, ~ë©ë‹ˆë‹¤, ~ì„¸ìš” ë“±). 2) ë“±ì¥ì¸ë¬¼ë“¤ì—ê²ŒëŠ” êµ¬ì²´ì ì¸ ì´ë¦„ì„ ì§€ì–´ì¤˜ (ì˜ˆ: ë¯¼ìˆ˜, ì§€í˜œ, í• ë¨¸ë‹ˆ, ê°•ì•„ì§€ ë½€ë¯¸ ë“±). ì ˆëŒ€ 'ì‚¬ìš©ì'ë¼ê³  ë¶€ë¥´ì§€ ë§ˆ. 3) ì´ì•¼ê¸°ëŠ” í•­ìƒ ë”°ëœ»í•˜ê³  êµìœ¡ì ì´ë©° ìƒìƒë ¥ì´ í’ë¶€í•˜ê²Œ ë§Œë“¤ì–´ì¤˜." }
];

// OpenAI Whisper Speech Recognition Setup with M Key Push-to-Talk
let isRecording = false;
let mediaRecorder;
let audioChunks = [];

// Global variable to store the selected voice once loaded
let koreanVoice = null;

// Function to load and set the Korean voice
function loadKoreanVoice() {
  const voices = speechSynthesis.getVoices();
  koreanVoice = voices.find(v => v.lang === 'ko-KR' && v.name.includes("Google"));
  if (!koreanVoice) {
    console.warn("Google Korean voice not found, falling back to any Korean voice.");
    koreanVoice = voices.find(v => v.lang === 'ko-KR');
  }
}

// Listen for voices to be loaded initially
if ('speechSynthesis' in window) {
  speechSynthesis.onvoiceschanged = loadKoreanVoice;
  // If voices are already loaded before the event fires (e.g., page refresh), load them
  if (speechSynthesis.getVoices().length > 0) {
    loadKoreanVoice();
  }
}

// Function for AI speech
function speakAiResponse(text) {
  if (!('speechSynthesis' in window)) {
    console.warn("Speech Synthesis API not supported.");
    return;
  }

  // If a speech is already in progress, stop it
  if (speechSynthesis.speaking) {
    speechSynthesis.cancel();
  }

  const utterance = new SpeechSynthesisUtterance(text);
  utterance.lang = 'ko-KR';

  // Use the globally set Korean voice
  if (koreanVoice) {
    utterance.voice = koreanVoice;
  } else {
    console.warn("Korean voice not yet loaded for speech. Speaking with default voice.");
    loadKoreanVoice();
    if (koreanVoice) {
      utterance.voice = koreanVoice;
    }
  }

  speechSynthesis.speak(utterance);
}

let isListening = false;

window.onload = function() {
  setupSpeechRecognition();
  
  const startBtn = document.getElementById('startStoryBtn');
  const storyDisplay = document.getElementById('storyDisplay');
  const continueBtn = document.getElementById('continueBtn');
  const aiStoryDisplay = document.getElementById('aiStoryDisplay');
  const statusDisplay = document.getElementById('statusDisplay');

  // Hide continue button initially
  if (continueBtn) continueBtn.style.display = 'none';

  startBtn.onclick = async function() {
    startBtn.disabled = true;
    storyDisplay.textContent = 'AIê°€ ì´ì•¼ê¸°ë¥¼ ë§Œë“œëŠ” ì¤‘...';
    aiStoryDisplay.textContent = '';
    if (statusDisplay) statusDisplay.textContent = '';
    storyMessages.length = 1; // Reset to system prompt only
    
    try {
      // AIì—ê²Œ ì²« ë¬¸ë‹¨ ìš”ì²­
      const response = await fetch("https://api.openai.com/v1/chat/completions", {
        method: "POST",
        headers: {
          "Authorization": `Bearer ${OPENAI_API_KEY}`,
          "Content-Type": "application/json"
        },
        body: JSON.stringify({
          model: "gpt-3.5-turbo",
          messages: storyMessages
        })
      });
      const data = await response.json();
      const firstParagraph = data.choices[0].message.content.trim();
      storyDisplay.textContent = firstParagraph;
      
      // Speak the first paragraph
      speakAiResponse(firstParagraph);
      
      storyMessages.push({role:'assistant', content: firstParagraph});
      
      // Show continue button and enable voice input
      if (continueBtn) {
        continueBtn.style.display = 'block';
        continueBtn.disabled = false;
      }
      
      if (statusDisplay) statusDisplay.textContent = 'ê³„ì†í•˜ê¸° ë²„íŠ¼ì„ ëˆŒëŸ¬ ìŒì„±ìœ¼ë¡œ ì´ì•¼ê¸°ë¥¼ ì´ì–´ê°€ì„¸ìš”!';
      
    } catch (error) {
      console.error('Error:', error);
      storyDisplay.textContent = 'ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.';
      startBtn.disabled = false;
    }
  };

  // Voice input handling
  if (continueBtn) {
    continueBtn.onclick = function() {
      if (!isListening) {
        startListening();
      }
    };
  }

  function setupSpeechRecognition() {
    // Show speech instructions
    const instructionsDiv = document.createElement('div');
    instructionsDiv.innerHTML = `
      <div style="background: #e3f2fd; padding: 12px; border-radius: 8px; margin: 15px 0; font-size: 0.95em; color: #1976d2;">
        <strong>ìŒì„± ì…ë ¥:</strong> M í‚¤ë¥¼ ëˆ„ë¥´ê³  ìˆëŠ” ë™ì•ˆ ë§í•˜ì„¸ìš”. í‚¤ë¥¼ ë–¼ë©´ ìŒì„±ì´ ì¸ì‹ë©ë‹ˆë‹¤.
      </div>
    `;
    const container = document.querySelector('.twentyq-container, .game-container');
    if (container) {
      container.insertBefore(instructionsDiv, container.children[1]);
    }

    // M key push-to-talk functionality
    document.addEventListener('keydown', function(event) {
      if (event.key === 'm' || event.key === 'M') {
        if (!isRecording && !isListening) {
          startRecording();
        }
      }
    });

    document.addEventListener('keyup', function(event) {
      if (event.key === 'm' || event.key === 'M') {
        if (isRecording) {
          stopRecording();
        }
      }
    });
  }

  async function startRecording() {
    if (isRecording) return;
    
    try {
      isRecording = true;
      isListening = true;
      audioChunks = [];
      
      if (continueBtn) continueBtn.disabled = true;
      updateSpeechStatus('ğŸ¤ Recording... (Release M key to stop)', '#ff5722');
      
      const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
      mediaRecorder = new MediaRecorder(stream);
      
      mediaRecorder.ondataavailable = function(event) {
        audioChunks.push(event.data);
      };
      
      mediaRecorder.onstop = async function() {
        const audioBlob = new Blob(audioChunks, { type: 'audio/wav' });
        await processAudio(audioBlob);
        
        // Stop all tracks
        stream.getTracks().forEach(track => track.stop());
      };
      
      mediaRecorder.start();
      
    } catch (err) {
      console.error('Error starting recording:', err);
      isRecording = false;
      isListening = false;
      if (continueBtn) continueBtn.disabled = false;
      updateSpeechStatus('Error: Could not access microphone', '#f44336');
    }
  }

  function stopRecording() {
    if (!isRecording || !mediaRecorder) return;
    
    isRecording = false;
    mediaRecorder.stop();
    updateSpeechStatus('ğŸ”„ Processing speech...', '#2196f3');
  }

  async function processAudio(audioBlob) {
    try {
      const arrayBuffer = await audioBlob.arrayBuffer();
      const buffer = window.electron.bufferFrom(arrayBuffer);
      
      const transcript = await window.electron.ipcRenderer.invoke('recognize-audio', buffer);
      
      if (transcript && transcript.trim()) {
        updateSpeechStatus(`ë“¤ì€ ë‚´ìš©: "${transcript.trim()}"`, '#4caf50');
        
        // Display user input in text
        const userInputDisplay = document.getElementById('userInputDisplay');
        if (userInputDisplay) {
          userInputDisplay.textContent = `í•™ìƒ: ${transcript.trim()}`;
          userInputDisplay.style.display = 'block';
        }
        
        await handleUserVoiceInput(transcript.trim());
      } else {
        updateSpeechStatus('âŒ No speech detected', '#ff9800');
      }
    } catch (err) {
      console.error('Speech recognition error:', err);
      updateSpeechStatus('âŒ Speech recognition failed', '#f44336');
    } finally {
      isListening = false;
      if (continueBtn) continueBtn.disabled = false;
    }
  }

  function updateSpeechStatus(message, color) {
    const statusDisplay = document.getElementById('statusDisplay');
    if (statusDisplay) {
      statusDisplay.textContent = message;
      statusDisplay.style.color = color;
      
      // Clear status after delay for success/error messages
      if (message.includes('âœ…') || message.includes('âŒ')) {
        setTimeout(() => {
          statusDisplay.textContent = '';
        }, 3000);
      }
    }
  }

  function startListening() {
    // This function is now just for backwards compatibility
    // The actual recording is handled by M key press
    updateSpeechStatus('ìŒì„± ì…ë ¥ì„ ì‚¬ìš©í•˜ë ¤ë©´ M í‚¤ë¥¼ ëˆŒëŸ¬ì„œ ë§í•˜ì„¸ìš”. í‚¤ë¥¼ ë–¼ë©´ ì¸ì‹ì´ ì‹œì‘ë©ë‹ˆë‹¤.', '#2196f3');
  }

  async function handleUserVoiceInput(userText) {
    if (!userText.trim()) return;
    
    if (statusDisplay) statusDisplay.textContent = 'AIê°€ ì´ì–´ì„œ ì“°ëŠ” ì¤‘...';
    aiStoryDisplay.textContent = '';
    
    storyMessages.push({role:'user', content: userText});
    
    try {
      const response = await fetch("https://api.openai.com/v1/chat/completions", {
        method: "POST",
        headers: {
          "Authorization": `Bearer ${OPENAI_API_KEY}`,
          "Content-Type": "application/json"
        },
        body: JSON.stringify({
          model: "gpt-3.5-turbo",
          messages: storyMessages
        })
      });
      const data = await response.json();
      const aiParagraph = data.choices[0].message.content.trim();
      aiStoryDisplay.textContent = aiParagraph;
      
      // Speak the AI response
      speakAiResponse(aiParagraph);
      
      storyMessages.push({role:'assistant', content: aiParagraph});
      
      if (statusDisplay) statusDisplay.textContent = 'ê³„ì†í•˜ê¸° ë²„íŠ¼ì„ ëˆŒëŸ¬ ìŒì„±ìœ¼ë¡œ ì´ì•¼ê¸°ë¥¼ ì´ì–´ê°€ì„¸ìš”!';
      
    } catch (error) {
      console.error('Error:', error);
      aiStoryDisplay.textContent = 'ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.';
      if (statusDisplay) statusDisplay.textContent = 'ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.';
    }
  }
};
